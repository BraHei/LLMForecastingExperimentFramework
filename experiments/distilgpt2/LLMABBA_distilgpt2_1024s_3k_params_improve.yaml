model_name: "distilgpt2"
model_parameters:
  max_new_tokens: 50
  temperature: 0.0
  repetition_penalty: 1.0
dataset_name: "kernelsynth"
dataset_params:
  num_series: 10
  max_kernels: 3
  sequence_lenght: 1024
preprocessor_name: "LLM-ABBA"
preprocessor_params:
  tol: 0.1
  alpha: 0.1
  init: 'kmeans'
  k: 52
  scl: 0.0
  max_len: 1
prompt_length_factor: 0.95
output_jsonl: "model_responses.jsonl"
data_analyzers: [MAE, MSE, RMSE]
